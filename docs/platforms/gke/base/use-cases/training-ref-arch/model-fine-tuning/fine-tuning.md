# Fine-tuning

Fine-tune a Gemma Instruction Tuned model using a flipkart processed catalog.
The dataset used for fine-tuning is generated by
[Llama 3.1 on Vertex AI](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3.1-405b-instruct-maas).
The fine-tuned model can be deployed with an inference serving engine.

The resulting fine-tuned model is, Built with Meta Llama 3.1, using the the data
prepared by the Llama 3.1 on Vertex AI API.

## Pre-requisites

- The
  [GKE Training reference implementation](/platforms/gke/base/use-cases/training-ref-arch/terraform/README.md)
  is deployed and configured in your repository.

  - The
    [model_fine_tuning](/platforms/gke/base/use-cases/training-ref-arch/terraform/model_fine_tuning/README.md)
    terraservice is deployed and configured.

## Before you begin

- A bucket containing the prepared data from the
  [Data Preparation example](/docs/platforms/gke/base/use-cases/training-ref-arch/model-fine-tuning/data-preparation.md)

## Preparation

> [!NOTE]  
> This guide is designed to be run on
> [Cloud Shell](https://cloud.google.com/shell) as it has all of the most of the
> required tools preinstalled.

- Source the environment configuration.

  ```shell
  source "${ACP_REPO_DIR}/platforms/gke/base/use-cases/training-ref-arch/_shared_config/scripts/set_environment_variables.sh"
  ```

- Verify the Hugging Face Hub **Read** access token secret.

  ```shell
  secret_version_found=$(gcloud secrets versions list "${huggingface_hub_access_token_read_secret_manager_secret_name}" --project="${huggingface_secret_manager_project_id}" 2>/dev/null | grep "enabled" | wc -l)
  echo
  if [[ ${secret_version_found} == 0 ]]; then
    echo "ERROR: Hugging Face Hub read token secret '${huggingface_hub_access_token_read_secret_manager_secret_name}' version is missing or not enabled! Please add the token to the secret."
  else
    echo "Hugging Face Hub read token secret '${huggingface_hub_access_token_read_secret_manager_secret_name}' version found."
  fi
  ```

## Build the container image

- Build the container image using Cloud Build and push the image to Artifact
  Registry

  ```shell
  export TF_PLUGIN_CACHE_DIR="${ACP_REPO_DIR}/.terraform.d/plugin-cache"
  cd ${ACP_REPO_DIR}/platforms/gke/base/use-cases/training-ref-arch/terraform/model_fine_tuning/images/fine_tuning && \
  rm -rf .terraform/ terraform.tfstate* && \
  terraform init && \
  terraform plan -input=false -out=tfplan && \
  terraform apply -input=false tfplan && \
  rm tfplan
  ```

## Run the job

- Configure the job

  ```shell
  "${ACP_REPO_DIR}/platforms/gke/base/use-cases/training-ref-arch/kubernetes-manifests/model-fine-tuning/fine-tuning/pytorch/configure.sh"
  ```

- Select an accelerator.

  - **NVIDIA A100**:

    ```shell
    export ACCELERATOR_TYPE="a100"
    ```

  - **NVIDIA H100 80GB**:

    ```shell
    export ACCELERATOR_TYPE="h100"
    ```

  - **NVIDIA Tesla L4 24GB**:

    ```shell
    export ACCELERATOR_TYPE="l4"
    ```

  Ensure that you have enough quota in your project to provision the selected
  accelerator type. For more information, see about viewing GPU quotas, see
  [Allocation quotas: GPU quota](https://cloud.google.com/compute/resource-usage#gpu_quota).

- Create the job

  ```shell
  kubectl --namespace=${mft_kubernetes_namespace} apply \
  --filename="${ACP_REPO_DIR}/platforms/gke/base/use-cases/training-ref-arch/kubernetes-manifests/model-fine-tuning/fine-tuning/pytorch/job-${ACCELERATOR_TYPE}.yaml" \
  --filename="${ACP_REPO_DIR}/platforms/gke/base/use-cases/training-ref-arch/kubernetes-manifests/model-fine-tuning/fine-tuning/pytorch/secretproviderclass-huggingface-tokens.yaml"
  ```

- Verify the completion of the job

  In the Google Cloud console, go to the
  [Logs Explorer](https://console.cloud.google.com/logs) page to run the
  following query to see the completion of the job.

  ```sh
  labels."k8s-pod/app"="finetune-job"
  textPayload: "finetune - INFO - ### Completed ###"
  ```

- After the fine-tuning job is successful, the model bucket should have a
  checkpoint folder created.

  ```sh
  gcloud storage ls gs://${mft_bucket_model_name}/model-data/model-gemma2/experiment
  ```

- Delete the job.

  ```sh
  kubectl --namespace=${mft_kubernetes_namespace} delete \
  --filename="${ACP_REPO_DIR}/platforms/gke/base/use-cases/training-ref-arch/kubernetes-manifests/model-fine-tuning/fine-tuning/pytorch/job-${ACCELERATOR_TYPE}.yaml" \
  --filename="${ACP_REPO_DIR}/platforms/gke/base/use-cases/training-ref-arch/kubernetes-manifests/model-fine-tuning/fine-tuning/pytorch/secretproviderclass-huggingface-tokens.yaml"
  ```

## Observability

Besides the logs and metrics provided by Google Cloud Observability, it's also
important to track the fine-tuning job and its results.

There are many existing options for this. As an example, we choose to use
[MLflow Tracking](https://mlflow.org/docs/latest/tracking.html) to keep track of
running the ML workloads. The MLflow Tracking is an API and UI for logging
parameters, code versions, metrics, and output files when running your machine
learning code and for later visualizing the results.

The
[model_fine_tuning terraservice](/platforms/gke/base/use-cases/training-ref-arch/terraform/model_fine_tuning)
in the
[GKE Training reference implementation ](platforms/gke/base/use-cases/training-ref-arch/terraform/README.md)
deploys MLflow Tracking for you.

You can run the following command to get the URL:

```sh
echo -e "\n${mft_kubernetes_namespace} MLFlow Tracking URL: ${mft_endpoint_mlflow_tracking_url}\n"
```

MLflow Tracking is protected by IAP. After you log in, you should see a page
similar to the following.

![mlflow-home](/docs/use-cases/model-fine-tuning-pipeline/fine-tuning/pytorch/images/mlflow-home.png)

All successful experiments should appear. If you click into a completed run, you
can see an overview page with metric tabs.

![mlflow-model-experiment](/docs/use-cases/model-fine-tuning-pipeline/fine-tuning/pytorch/images/mlflow-model-experiment.png)
