# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: obi
  namespace: replaced-by-kustomize
spec:
  replicatedJobs:
  - name: obi-job
    replicas: 1
    template:
      spec:
        parallelism: 2
        completions: 10
        completionMode: Indexed
        backoffLimit: 0
        template:
          metadata:
            annotations:
              gke-gcsfuse/cpu-limit: "0"
              gke-gcsfuse/ephemeral-storage-limit: "0"
              gke-gcsfuse/memory-limit: "0"
              gke-gcsfuse/volumes: "true"
            labels:
              ai.gke.io/model: replaced-by-kustomize
              app: vllm
          spec:
            nodeSelector:
              cloud.google.com/compute-class: replaced-by-kustomize
            serviceAccountName: replaced-by-kustomize
            restartPolicy: OnFailure
            volumes:
              - emptyDir:
                  medium: Memory
                name: dev-shm
              - csi:
                  driver: gcsfuse.csi.storage.gke.io
                  volumeAttributes:
                    bucketName: cloud-storage-bucket-name
                    mountOptions: metadata-cache:ttl-secs:-1,metadata-cache:stat-cache-max-size-mb:-1,metadata-cache:type-cache-max-size-mb:-1,metadata-cache:negative-ttl-secs:0,file-cache:max-size-mb:-1,file-cache:cache-file-for-range-read:true,file-cache:enable-parallel-downloads:true,implicit-dirs,file-system:kernel-list-cache-ttl-secs:-1,only-dir:replaced-by-kustomize
                    skipCSIBucketAccessCheck: "true"
                name: huggingface-hub-model-bucket
              - emptyDir:
                  medium: Memory
                name: gke-gcsfuse-cache
              - emptyDir:
                  medium: Memory
                name: gke-gcsfuse-tmp
              - emptyDir:
                  medium: Memory
                name: gke-gcsfuse-buffer
            initContainers:
            - args:
              - |
                echo "########### $(date) - Starting parallel-fetch-safetensors for model: ${MODEL_ID}"
                ls -alR /gcs
                find /gcs/${MODEL_ID}/*safetensors -type f | xargs -I {} -P 15 sh -c 'echo "########### $(date) - Fetching: {}"; dd if={} of=/dev/null'
                echo "########### $(date) - Finished parallel-fetch-safetensors"
                sleep infinity
              command: ["/bin/sh", "-c"]
              env:
                - name: MODEL_ID
                  valueFrom:
                    configMapKeyRef:
                      key: MODEL_ID
                      name: runtime
              image: busybox
              name: fetch-safetensors
              restartPolicy: Always
              volumeMounts:
              - name: huggingface-hub-model-bucket
                mountPath: /gcs
                readOnly: true
            - args:
              - "--disable-log-requests"
              - "--gpu-memory-utilization=$(GPU_MEMORY_UTILIZATION)"
              - "--model=/gcs/$(MODEL_ID)"
              - "--tensor-parallel-size=$(TENSOR_PARALLEL_SIZE)"
              - "--trust-remote-code"
              - "--max-model-len=$(MAX_MODEL_LEN)"
              command:
              - python3
              - "-m"
              - vllm.entrypoints.openai.api_server
              env:
                - name: GPU_MEMORY_UTILIZATION
                  valueFrom:
                    configMapKeyRef:
                      key: GPU_MEMORY_UTILIZATION
                      name: runtime
                - name: LD_LIBRARY_PATH
                  value: ${LD_LIBRARY_PATH}:/usr/local/nvidia/lib64
                - name: MAX_MODEL_LEN
                  valueFrom:
                    configMapKeyRef:
                      key: MAX_MODEL_LEN
                      name: runtime
                - name: MODEL_ID
                  valueFrom:
                    configMapKeyRef:
                      key: MODEL_ID
                      name: runtime
                - name: TENSOR_PARALLEL_SIZE
                  valueFrom:
                    configMapKeyRef:
                      key: TENSOR_PARALLEL_SIZE
                      name: runtime
                - name: VLLM_XLA_CACHE_PATH
                  value: /data
              image: replaced-by-kustomize
              imagePullPolicy: Always
              name: inference-server
              ports:
                - containerPort: 8000
                  name: metrics
              readinessProbe:
                failureThreshold: 6000
                httpGet:
                  path: /health
                  port: 8000
                  scheme: HTTP
                initialDelaySeconds: 60
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 1
              resources:
                limits:
                  cpu: "64"
                  memory: 320G
                  nvidia.com/gpu: "2"
                requests:
                  cpu: "64"
                  memory: 320G
                  nvidia.com/gpu: "2"
              restartPolicy: Always
              volumeMounts:
                - mountPath: /dev/shm
                  name: dev-shm
                - mountPath: /gcs
                  name: huggingface-hub-model-bucket
                  readOnly: true
            containers:
            - env:
              - name: PYTHONUNBUFFERED
                value: "1"
              - name: DATASET_BUCKET_NAME
                valueFrom:
                  configMapKeyRef:
                    key: DATASET_BUCKET_NAME
                    name: offline-batch-worker
              - name: VLLM_API_ENDPOINT
                valueFrom:
                  configMapKeyRef:
                    key: VLLM_API_ENDPOINT
                    name: offline-batch-worker
              - name: CONCURRENT_REQUESTS
                valueFrom:
                  configMapKeyRef:
                    key: CONCURRENT_REQUESTS
                    name: offline-batch-worker
              image: replaced-by-kustomize
              imagePullPolicy: Always
              name: worker
