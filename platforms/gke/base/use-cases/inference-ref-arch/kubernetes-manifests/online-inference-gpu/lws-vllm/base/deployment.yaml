# Copyright 2025 Google LLC

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

# https://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: vllm
  namespace: replaced-by-kustomize
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    restartPolicy: RecreateGroupOnPodRestart
    leaderTemplate:
      metadata:
        labels:
          app: replaced-by-kustomize
          role: leader
      spec:
        containers:
          - name: vllm-leader
            image: vllm/vllm-openai:v0.10.1.1
            env:
              - name: GPU_MEMORY_UTILIZATION
                valueFrom:
                  configMapKeyRef:
                    key: GPU_MEMORY_UTILIZATION
                    name: runtime
              - name: LD_LIBRARY_PATH
                value: ${LD_LIBRARY_PATH}:/usr/local/nvidia/lib64
              - name: MAX_MODEL_LEN
                valueFrom:
                  configMapKeyRef:
                    key: MAX_MODEL_LEN
                    name: runtime
              - name: MODEL_ID
                valueFrom:
                  configMapKeyRef:
                    key: MODEL_ID
                    name: runtime
              - name: TENSOR_PARALLEL_SIZE
                valueFrom:
                  configMapKeyRef:
                    key: TENSOR_PARALLEL_SIZE
                    name: runtime
              - name: VLLM_XLA_CACHE_PATH
                value: /data
            command:
              - sh
              - -c
              - "bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size=$(LWS_GROUP_SIZE);
                python3 -m vllm.entrypoints.openai.api_server --port 8080 --model /gcs/$(MODEL_ID) --tensor-parallel-size $(TENSOR_PARALLEL_SIZE) --pipeline-parallel-size $(PIPELINE_PARALLEL_SIZE) --trust-remote-code --max-model-len $(MAX_MODEL_LEN)"
            resources:
              limits:
                nvidia.com/gpu: "8"
            ports:
              - containerPort: 8080
            readinessProbe:
              tcpSocket:
                port: 8080
              initialDelaySeconds: 15
              periodSeconds: 10
            volumeMounts:
              - mountPath: /dev/shm
                name: dev-shm
              - mountPath: /gcs
                name: huggingface-hub-model-bucket
                readOnly: true
        serviceAccountName: replaced-by-kustomize
        volumes:
          - emptyDir:
              medium: Memory
            name: dev-shm
          - csi:
              driver: gcsfuse.csi.storage.gke.io
              volumeAttributes:
                bucketName: cloud-storage-bucket-name
                mountOptions: metadata-cache:ttl-secs:-1,metadata-cache:stat-cache-max-size-mb:-1,metadata-cache:type-cache-max-size-mb:-1,metadata-cache:negative-ttl-secs:0,file-cache:max-size-mb:-1,file-cache:cache-file-for-range-read:true,file-cache:enable-parallel-downloads:true,implicit-dirs,file-system:kernel-list-cache-ttl-secs:-1,only-dir:replaced-by-kustomize
                skipCSIBucketAccessCheck: "true"
            name: huggingface-hub-model-bucket
          - emptyDir:
              medium: Memory
            name: gke-gcsfuse-cache
          - emptyDir:
              medium: Memory
            name: gke-gcsfuse-tmp
          - emptyDir:
              medium: Memory
            name: gke-gcsfuse-buffer
    workerTemplate:
      spec:
        containers:
          - name: vllm-worker
            image: vllm/vllm-openai:v0.10.1.1
            command:
              - sh
              - -c
              - "bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker --ray_address=$(LWS_LEADER_ADDRESS)"
            resources:
              limits:
                nvidia.com/gpu: "8"
            env:
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-secret
                    key: hf_api_token
              - name: LD_LIBRARY_PATH
                value: ${LD_LIBRARY_PATH}:/usr/local/nvidia/lib64
              - name: VLLM_XLA_CACHE_PATH
                value: /data
            volumeMounts:
              - mountPath: /dev/shm
                name: dev-shm
              - mountPath: /gcs
                name: huggingface-hub-model-bucket
                readOnly: true
        serviceAccountName: replaced-by-kustomize
        volumes:
          - emptyDir:
              medium: Memory
            name: dev-shm
          - csi:
              driver: gcsfuse.csi.storage.gke.io
              volumeAttributes:
                bucketName: cloud-storage-bucket-name
                mountOptions: metadata-cache:ttl-secs:-1,metadata-cache:stat-cache-max-size-mb:-1,metadata-cache:type-cache-max-size-mb:-1,metadata-cache:negative-ttl-secs:0,file-cache:max-size-mb:-1,file-cache:cache-file-for-range-read:true,file-cache:enable-parallel-downloads:true,implicit-dirs,file-system:kernel-list-cache-ttl-secs:-1,only-dir:replaced-by-kustomize
                skipCSIBucketAccessCheck: "true"
            name: huggingface-hub-model-bucket
          - emptyDir:
              medium: Memory
            name: gke-gcsfuse-cache
          - emptyDir:
              medium: Memory
            name: gke-gcsfuse-tmp
          - emptyDir:
              medium: Memory
            name: gke-gcsfuse-buffer